{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204562f5-feeb-4fee-84ab-414e718d32e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2563 images belonging to 2 classes.\n",
      "Found 640 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumy\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7527 - loss: 0.5394 - precision: 0.7527 - recall: 0.7527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumy\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 3s/step - accuracy: 0.7540 - loss: 0.5367 - precision: 0.7540 - recall: 0.7540 - val_accuracy: 0.9953 - val_loss: 0.0415 - val_precision: 0.9953 - val_recall: 0.9953\n",
      "Epoch 2/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 3s/step - accuracy: 0.9698 - loss: 0.0893 - precision: 0.9698 - recall: 0.9698 - val_accuracy: 0.9937 - val_loss: 0.0239 - val_precision: 0.9937 - val_recall: 0.9937\n",
      "Epoch 3/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 3s/step - accuracy: 0.9856 - loss: 0.0518 - precision: 0.9856 - recall: 0.9856 - val_accuracy: 0.9969 - val_loss: 0.0167 - val_precision: 0.9969 - val_recall: 0.9969\n",
      "Epoch 4/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 3s/step - accuracy: 0.9888 - loss: 0.0339 - precision: 0.9888 - recall: 0.9888 - val_accuracy: 0.9984 - val_loss: 0.0177 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 5/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.9891 - loss: 0.0290 - precision: 0.9891 - recall: 0.9891 - val_accuracy: 0.9953 - val_loss: 0.0200 - val_precision: 0.9953 - val_recall: 0.9953\n",
      "Epoch 6/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 3s/step - accuracy: 0.9917 - loss: 0.0291 - precision: 0.9917 - recall: 0.9917 - val_accuracy: 0.9984 - val_loss: 0.0130 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 7/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 3s/step - accuracy: 0.9937 - loss: 0.0181 - precision: 0.9937 - recall: 0.9937 - val_accuracy: 0.9953 - val_loss: 0.0148 - val_precision: 0.9953 - val_recall: 0.9953\n",
      "Epoch 8/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 3s/step - accuracy: 0.9969 - loss: 0.0131 - precision: 0.9969 - recall: 0.9969 - val_accuracy: 0.9984 - val_loss: 0.0104 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 9/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 3s/step - accuracy: 0.9985 - loss: 0.0113 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9984 - val_loss: 0.0066 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 10/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 3s/step - accuracy: 0.9995 - loss: 0.0068 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9984 - val_loss: 0.0128 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 11/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 3s/step - accuracy: 0.9979 - loss: 0.0079 - precision: 0.9979 - recall: 0.9979 - val_accuracy: 0.9984 - val_loss: 0.0125 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 12/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 3s/step - accuracy: 0.9996 - loss: 0.0055 - precision: 0.9996 - recall: 0.9996 - val_accuracy: 0.9984 - val_loss: 0.0114 - val_precision: 0.9984 - val_recall: 0.9984\n",
      "Epoch 13/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 3s/step - accuracy: 0.9983 - loss: 0.0060 - precision: 0.9983 - recall: 0.9983 - val_accuracy: 0.9969 - val_loss: 0.0141 - val_precision: 0.9969 - val_recall: 0.9969\n",
      "Epoch 14/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0044 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9984 - val_loss: 0.0103 - val_precision: 0.9984 - val_recall: 0.9984\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "class AIImageClassifier:\n",
    "    def __init__(self, img_height=224, img_width=224, channels=3):\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.channels = channels\n",
    "        self.model = self.build_model()\n",
    "        self.class_names = None\n",
    "\n",
    "    def build_model(self):\n",
    "        base_model = ResNet50V2(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(self.img_height, self.img_width, self.channels)\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        model = models.Sequential([\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy', \n",
    "                     tf.keras.metrics.Precision(), \n",
    "                     tf.keras.metrics.Recall()]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def prepare_data(self, data_dir):\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "        \n",
    "        train_generator = datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(self.img_height, self.img_width),  # Resize all images to specified dimensions\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        validation_generator = datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=(self.img_height, self.img_width),  # Resize all images to specified dimensions\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Store class names\n",
    "        self.class_names = list(train_generator.class_indices.keys())\n",
    "        \n",
    "        return train_generator, validation_generator\n",
    "\n",
    "    def train(self, data_dir, epochs=50):\n",
    "        train_generator, validation_generator = self.prepare_data(data_dir)\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_accuracy', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        # Ensure class names are set\n",
    "        if self.class_names is None:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = load_img(image_path, target_size=(self.img_height, self.img_width))\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(img_array)\n",
    "        predicted_class_index = np.argmax(prediction)\n",
    "        predicted_class = self.class_names[predicted_class_index]\n",
    "        confidence = prediction[0][predicted_class_index] * 100\n",
    "        \n",
    "        return f\"{predicted_class} (Confidence: {confidence:.2f}%)\"\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == '__main__':\n",
    "    # Adjust dimensions to match your smallest dataset\n",
    "    classifier = AIImageClassifier(img_height=150, img_width=150)\n",
    "    classifier.train(r\"C:\\Users\\soumy\\Downloads\\ai-real\\AI-face-detection-Dataset\")\n",
    "    \n",
    "    # Example prediction\n",
    "    # result = classifier.predict(\"path/to/test/image.jpg\")\n",
    "    # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1546c4a2-4f38-416e-8eb7-732df72fe1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "AI Generated (Confidence: 94.52%)\n"
     ]
    }
   ],
   "source": [
    "    # Predict single image\n",
    "    result = classifier.predict(r\"C:\\\\Users\\\\soumy\\\\Downloads\\\\vecteezy_ai-generated-indian-female-student_39334804.jpg\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c66b1d0-67b4-4115-90b8-90a82df8d8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "AI Generated (Confidence: 85.57%)\n"
     ]
    }
   ],
   "source": [
    "    # Predict single image\n",
    "    result = classifier.predict(r\"C:\\\\Users\\\\soumy\\\\Downloads\\\\ai-generated-image.jpg\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e123c3a6-445e-4c21-936b-72931e678d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "AI Generated (Confidence: 53.08%)\n"
     ]
    }
   ],
   "source": [
    "    # Predict single image\n",
    "    result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\Armaan_Malik_2016.jpg\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd9659e-40ac-4855-9bd8-b770da3a8804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Real Image (Confidence: 97.87%)\n"
     ]
    }
   ],
   "source": [
    "# Predict single image\n",
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\WhatsApp Image 2025-01-09 at 11.01.21.jpeg\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41cd6fe-7b85-4ea4-a50c-46bfa4d792a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Real Image (Confidence: 88.68%)\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\WhatsApp Image 2025-01-09 at 11.01.22.jpeg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3124458f-261d-401c-a6a8-224f40b1528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "Real Image (Confidence: 99.94%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\ai-real\\AI-face-detection-Dataset\\Real Image\\non-child-978.png\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe9ef33c-6520-4785-acbd-b898666bfb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "Real Image (Confidence: 98.94%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\images.jpg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2746847-0241-4600-a7ec-2f32653d7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "AI Generated (Confidence: 93.46%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\no-filters-closeup-portrait-young-260nw-2103259805.webp\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "809304ef-50a5-4129-80ac-55320dd26d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Real Image (Confidence: 82.94%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\OneDrive\\Pictures\\Camera Roll\\WIN_20250124_22_29_33_Pro.jpg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "466524f4-212b-45f8-95e5-410724dbbfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "AI Generated (Confidence: 64.81%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\premium_photo-1661585987573-c5873c7e258e.jpg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a40769b4-404a-433a-83a1-4f5bf9d48e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Real Image (Confidence: 99.45%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\pexels-photo-2379005.jpeg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f99c3778-5aba-4af2-b1d7-ece2ca919e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "Real Image (Confidence: 69.71%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\v3_0924096 ai generated.jpg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "258e4d1f-5859-4069-9ddf-f190d09a98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "AI Generated (Confidence: 92.72%)\n"
     ]
    }
   ],
   "source": [
    "result = classifier.predict(r\"C:\\Users\\soumy\\Downloads\\v3_0344789 ai generated.jpg\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b92473-9c88-4e89-9ffb-7cb315384861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
